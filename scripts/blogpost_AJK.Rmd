---
title: "War by the neighbor through the lenses of Hungarian news outlets"
author: "Adam Jozsef Kovacs"
date: "5/11/2022"
output: html_document
---

```{r set chunk options, include=FALSE}
#setting chunk options
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE, eval = TRUE)
```

## Introduction 

Since Russian troops entered the territory of Ukraine and consequently started a war on the 24th February, 2022, thousands of civilians lost their lives and millions were forced to flee the country. The goal of this blogpost is to grasp interesting patterns in the coverage of the war through an in-depth quantitative text analysis of all articles written on the matter by four pro-government Hungarian news portals. To this end, using web scraping, the texts of more than 2000 articles were collected. 

The main research question of the project is what the communication of these mediums looked like before and after April 3rd 2022, when the Hungarian elections were held and around which time the news about the Bucha massacre became public. My impression dictates the main hypothesis of the post, which is that there has been a change in the coverage. I feel that it focuses much less on the humanitarian aid, and how the Hungarians are pacifists and much more on the painful consequences and how we have to be cautious with breaking all ties with the Russians as we are so dependent on their oil and gas. 

All artifacts of this project including the R script used for scraping the articles can be found in [THIS](https://github.com/kovaad/DS3_project) github repository while the data is uploaded in [THIS](https://drive.google.com/drive/folders/1cMQixfuBkXMLztMrB60vyCPrYCxUBCO3?usp=sharing) Google Drive folder as it was too large to put on github. After downloading it, please put it in the data folder to make the code reproducible.

```{r read in, echo = FALSE}
#clear environment
#rm(list = ls())

#load packages
if (!require("pacman")) {
  install.packages("pacman")
}
pacman::p_load(dplyr,tidyverse, lubridate, tidytext, ggplot2, mediumr)#, quanteda, quanteda.textstats, ggrepel,text2vec, topicmodels,ggfortify,ggwordcloud)

#check out custom theme
source("theme_adam.R")
```

## Data collection

For the data collection, I decided to scrape not only one, but four news outlets coverage on the war. During this process, I first scraped all the titles, the dates (day-month-year) on which the articles were written (this had to be standardized) and the links. Then I went through all these links and scraped the actual content (body) of the articles and merged it back to the dataframe. 

After binding together all the articles, I read it in for the analysis. The first preprocessing step is to get rid of whitespaces and create an additional dummy variable that signals whether the article was written before or after 3rd April, 2022. For the justification of this date as a milestone refer back to the introduction. 

```{r initial cleaning, echo=FALSE}

#read in data
df <- read_csv("../data/full_df_correct.csv")

#create target variable of before and after the elections
#rename body for text
#remove any unnecessary whitespces
df <- df |> 
  mutate(
    label = ifelse(dates < ymd("2022-4-3"), "before", "after") 
  ) |> 
  rename(text = body) |> 
  mutate(
    text = stringr::str_trim(text),
    text = stringr::str_squish(text)
  )

```

Next, I tokenize all the articles and calculate how many tokens each article consists of. I append this information back to the main dataframe and remove all articles that have less than 10 tokens as they are very unlikely to contain any interesting results to my analysis. 

```{r tokenize and count, echo=FALSE}
#create tidy tokens dataframe using tidytext
tokens <- df |> 
  unnest_tokens(word, text)

#count tokens by article
tok_count <- tokens |> 
  group_by(dates,links) |> 
  summarise( 
    sum_tokens = n()
  )

#add to dataframe this information
df <- left_join(df, tok_count[, names(tok_count) != "dates"], by = "links")

#remove those that contain less than 10 words
df <- subset(df, !(sum_tokens <= 10))

```

The first descriptive plot depicts the number of articles by news portal. As we can see the majority of the articles are from [magyar nemzet](https://magyarnemzet.hu), namely 1348. In their case, these are all the articles with the tag #warinukraine in Hungarian. This is followed by [mandiner](https://mandiner.hu) with 537 articles, where the tag was #Russian-Ukranianwar in Hungarian. The third source is [szoljon](https://www.szoljon.hu), which is the regional news portal for JÃ¡sz-Nagykun-Szolnok county. It is well-known that all the regional news portal are owned by the same pro-government consortium, so it was chosen to represent that segment of the media. In their case all the articles with the tag #ukraine were pulled from around the start of the Russian invasion. Finally, the fourth news portal is [pesti sracok](https://pestisracok.hu), which is also a pro-government, more radical news portal, where again all articles with the tag #ukraine were collected. 

```{r portal count, include = FALSE}
#create chart of number of articles by news portal
count_by_portal <- df |> group_by(name) |> summarise(count = n()) |> arrange(desc(count))

ggplot(count_by_portal, aes(reorder(name,-count), count)) +
  geom_bar(stat = "identity") +
  labs(
    y = "Number of articles",
    x = NULL
  ) +
  geom_text(data=count_by_portal,aes(label=count,y=count),vjust=-0.5) +
  theme_adam()

ggsave("../images/count_by_portal.png")

```

Next, I was also curious about the time dimension of my data, both the number of articles that were written on these portals and the length of them. First, I visualize the number of articles appeared. 

```{r number of articles over time, include = FALSE}
#get number of articles and average length of articles by date
overtime <- tok_count %>%
  group_by(dates) %>% 
  summarise( 
    n_articles = n(),
    avg_tokens = mean(sum_tokens)
  )


#plot number of articles in dataset over time
arrows <- 
  tibble(
    x1 = c(ymd("2022-2-8"), ymd("2022-4-15")),
    x2 =  c(ymd("2022-2-24"),ymd("2022-4-3")),
    y1 = c(55, 45), 
    y2 = c(65, 48)
  )

ggplot(overtime, aes(dates, n_articles)) +
  geom_line() +
  labs(
    y = "Number of articles",
    x = NULL
  ) + 
  geom_vline(xintercept=ymd("2022-2-24"), linetype="dashed", 
             color = "red", size=1) + 
  geom_vline(xintercept=ymd("2022-4-3"), linetype="dashed", 
             color = "red", size=1) +
  ggplot2::annotate("text", x = ymd("2022-2-8"), y = 52, label = "Russian invasion of \n Ukraine") +
  ggplot2::annotate("text", x = ymd("2022-4-19"), y = 48, label = "Elections/\nBucha massacre") +
  geom_curve(
    data = arrows, aes(x = x1, y = y1, xend = x2, yend = y2),
    arrow = arrow(length = unit(0.08, "inch")), size = 0.5,
    color = "gray20", curvature = -0.3) +
  theme_adam()

ggsave("../images/count_over_time.png")

```

As we can see from the figure, the 

Then, I look at the length of articles. 

```{r evolution of length of articles, include = FALSE}
#plot of evolution of length of articles
arrows <- 
  tibble(
    x1 = c(ymd("2022-2-3"), ymd("2022-4-23")),
    x2 =  c(ymd("2022-2-24"),ymd("2022-4-3")),
    y1 = c(650, 200), 
    y2 = c(700, 250)
  )

ggplot(overtime, aes(dates, avg_tokens)) +
  geom_line() +
  labs(
    y = "Average length of articles",
    x = NULL
  ) + 
  geom_vline(xintercept=ymd("2022-2-24"), linetype="dashed", 
             color = "red", size=1) + 
  geom_vline(xintercept=ymd("2022-4-3"), linetype="dashed", 
             color = "red", size=1) +
  ggplot2::annotate("text", x = ymd("2022-2-2"), y = 620, label = "Russian invasion of \n Ukraine") +
  ggplot2::annotate("text", x = ymd("2022-4-23"), y = 230, label = "Elections/\nBucha massacre") +
  geom_curve(
    data = arrows, aes(x = x1, y = y1, xend = x2, yend = y2),
    arrow = arrow(length = unit(0.08, "inch")), size = 0.5,
    color = "gray20", curvature = -0.3) +
  theme_adam()

ggsave("../images/length_over_time.png")

```









